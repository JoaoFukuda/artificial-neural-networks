Alpha: 0.2

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 2
Activator function in the hidden layer: ReLu
Input weights for perceptron 1 of hidden layer: 0.36203295
                                                0.55901945
                                                0.4319442 (bias)

Input weights for perceptron 2 of hidden layer: 0.8638383
                                                0.45769137
                                                0.7163305 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.48428148
                                                0.27697688
                                                0.4200654 (bias)

-------------------------------Test #1-------------------------------
Alpha: 0.2

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 5
Activator function in the hidden layer: ReLu
Input weights for perceptron 1 of hidden layer: 0.6817482
                                                0.7503453
                                                0.4466399 (bias)

Input weights for perceptron 2 of hidden layer: 0.40122497
                                                0.39911598
                                                0.0057033896 (bias)

Input weights for perceptron 3 of hidden layer: 0.4434917
                                                0.71160597
                                                0.37917817 (bias)

Input weights for perceptron 4 of hidden layer: 0.5720084
                                                0.57001424
                                                0.32714385 (bias)

Input weights for perceptron 5 of hidden layer: 0.99574524
                                                0.44091123
                                                0.63169116 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.6952728
                                                0.074490964
                                                0.43201572
                                                0.8427093
                                                0.08382112
                                                0.56231517 (bias)

-------------------------------Test #2-------------------------------
Alpha: 0.2

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 6
Activator function in the hidden layer: ReLu
Input weights for perceptron 1 of hidden layer: 0.75231993
                                                0.8551064
                                                0.19739419 (bias)

Input weights for perceptron 2 of hidden layer: 0.08757633
                                                0.8474727
                                                0.7126038 (bias)

Input weights for perceptron 3 of hidden layer: 0.76248705
                                                0.912596
                                                0.081098735 (bias)

Input weights for perceptron 4 of hidden layer: 0.4407671
                                                0.71508205
                                                0.30397648 (bias)

Input weights for perceptron 5 of hidden layer: 0.20710051
                                                0.7119889
                                                0.36109912 (bias)

Input weights for perceptron 6 of hidden layer: 0.35946184
                                                0.6487057
                                                0.77101547 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.9575277
                                                0.9200011
                                                0.061395645
                                                0.40139502
                                                0.35019368
                                                0.97865415
                                                0.20063609 (bias)

-------------------------------Test #3-------------------------------
Alpha: 0.2

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 7
Activator function in the hidden layer: ReLu
Input weights for perceptron 1 of hidden layer: 0.16434431
                                                0.13703257
                                                0.06973797 (bias)

Input weights for perceptron 2 of hidden layer: 0.21029174
                                                0.23797232
                                                0.6312041 (bias)

Input weights for perceptron 3 of hidden layer: 0.26185334
                                                0.44119352
                                                0.14256781 (bias)

Input weights for perceptron 4 of hidden layer: 0.69250524
                                                0.60877144
                                                0.38199025 (bias)

Input weights for perceptron 5 of hidden layer: 0.3564362
                                                0.09966302
                                                0.22870374 (bias)

Input weights for perceptron 6 of hidden layer: 0.9597733
                                                0.86875296
                                                0.9686785 (bias)

Input weights for perceptron 7 of hidden layer: 0.34921634
                                                0.91352284
                                                0.018504739 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.17222208
                                                0.37740183
                                                0.12247306
                                                0.60537744
                                                0.10433859
                                                0.38582915
                                                0.7111568
                                                0.9678149 (bias)

-------------------------------Test #4-------------------------------
Alpha: 0.2

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 8
Activator function in the hidden layer: ReLu
Input weights for perceptron 1 of hidden layer: 0.66964656
                                                0.4516288
                                                0.4955775 (bias)

Input weights for perceptron 2 of hidden layer: 0.39892584
                                                0.7127356
                                                0.31303948 (bias)

Input weights for perceptron 3 of hidden layer: 0.07486683
                                                0.13695914
                                                0.17980194 (bias)

Input weights for perceptron 4 of hidden layer: 0.96335274
                                                0.08400357
                                                0.2607838 (bias)

Input weights for perceptron 5 of hidden layer: 0.54156536
                                                0.18314666
                                                0.7550209 (bias)

Input weights for perceptron 6 of hidden layer: 0.3160993
                                                0.5933005
                                                0.49051923 (bias)

Input weights for perceptron 7 of hidden layer: 0.7706791
                                                0.8888541
                                                0.60433006 (bias)

Input weights for perceptron 8 of hidden layer: 0.2502098
                                                0.41626173
                                                0.9694731 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.67572844
                                                0.74819934
                                                0.59830916
                                                0.78319174
                                                0.1415928
                                                0.59965384
                                                0.9334133
                                                0.8243159
                                                0.3742031 (bias)

-------------------------------Test #5-------------------------------
Alpha: 0.2

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 9
Activator function in the hidden layer: ReLu
Input weights for perceptron 1 of hidden layer: 0.11451769
                                                0.23131883
                                                0.76684314 (bias)

Input weights for perceptron 2 of hidden layer: 0.50878143
                                                0.31354266
                                                0.031249166 (bias)

Input weights for perceptron 3 of hidden layer: 0.51691437
                                                0.64057493
                                                0.95226836 (bias)

Input weights for perceptron 4 of hidden layer: 0.68193305
                                                0.28429425
                                                0.089819014 (bias)

Input weights for perceptron 5 of hidden layer: 0.71133035
                                                0.5900835
                                                0.97475594 (bias)

Input weights for perceptron 6 of hidden layer: 0.9810854
                                                0.038203716
                                                0.5837478 (bias)

Input weights for perceptron 7 of hidden layer: 0.83053094
                                                0.8836508
                                                0.3802315 (bias)

Input weights for perceptron 8 of hidden layer: 0.57420975
                                                0.30118382
                                                0.94429195 (bias)

Input weights for perceptron 9 of hidden layer: 0.14045668
                                                0.26090026
                                                0.17573851 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.095188916
                                                0.017184496
                                                0.19440126
                                                0.58102787
                                                0.7625927
                                                0.33731812
                                                0.04660487
                                                0.37151033
                                                0.025321364
                                                0.9522301 (bias)

-------------------------------Test #6-------------------------------
Alpha: 0.2

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 9
Activator function in the hidden layer: ReLu
Input weights for perceptron 1 of hidden layer: 0.84968877
                                                0.8138833
                                                0.9448646 (bias)

Input weights for perceptron 2 of hidden layer: 0.34369993
                                                0.002261281
                                                0.6085355 (bias)

Input weights for perceptron 3 of hidden layer: 0.09497994
                                                0.19818348
                                                0.58126473 (bias)

Input weights for perceptron 4 of hidden layer: 0.08840591
                                                0.5574546
                                                0.71944875 (bias)

Input weights for perceptron 5 of hidden layer: 0.7957212
                                                0.39052647
                                                0.85771054 (bias)

Input weights for perceptron 6 of hidden layer: 0.20701623
                                                0.24359113
                                                0.88299996 (bias)

Input weights for perceptron 7 of hidden layer: 0.5831911
                                                0.5784092
                                                0.9427746 (bias)

Input weights for perceptron 8 of hidden layer: 0.11324686
                                                0.7706245
                                                0.05544716 (bias)

Input weights for perceptron 9 of hidden layer: 0.45752728
                                                0.39393407
                                                0.2424348 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: ReLu
Input weights for perceptron 1 of output layer: 0.3217197
                                                0.20880818
                                                0.9214389
                                                0.2646926
                                                0.8279246
                                                0.5529888
                                                0.2651776
                                                0.4370824
                                                0.30001986
                                                0.32809258 (bias)

-------------------------------Test #7-------------------------------
Alpha: 0.2

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 9
Activator function in the hidden layer: ReLu
Input weights for perceptron 1 of hidden layer: 0.38963747
                                                0.09939748
                                                0.10382652 (bias)

Input weights for perceptron 2 of hidden layer: 0.2578568
                                                0.17725027
                                                0.2895617 (bias)

Input weights for perceptron 3 of hidden layer: 0.99102193
                                                0.4177379
                                                0.74888444 (bias)

Input weights for perceptron 4 of hidden layer: 0.10956973
                                                0.26902694
                                                0.61744463 (bias)

Input weights for perceptron 5 of hidden layer: 0.53532064
                                                0.73886096
                                                0.41839427 (bias)

Input weights for perceptron 6 of hidden layer: 0.92322314
                                                0.0039801598
                                                0.64965373 (bias)

Input weights for perceptron 7 of hidden layer: 0.73260486
                                                0.046983898
                                                0.041634083 (bias)

Input weights for perceptron 8 of hidden layer: 0.34707218
                                                0.30879378
                                                0.99679637 (bias)

Input weights for perceptron 9 of hidden layer: 0.0433439
                                                0.6101257
                                                0.45081854 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.82406646
                                                0.9904244
                                                0.29958713
                                                0.30109745
                                                0.47185212
                                                0.8174899
                                                0.2560944
                                                0.7508178
                                                0.36197704
                                                0.63652223 (bias)

-------------------------------Test #8-------------------------------
Alpha: 0.2

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 9
Activator function in the hidden layer: Sigmoid
Input weights for perceptron 1 of hidden layer: 0.67125195
                                                0.48792434
                                                0.6263681 (bias)

Input weights for perceptron 2 of hidden layer: 0.5343896
                                                0.06371659
                                                0.0017693639 (bias)

Input weights for perceptron 3 of hidden layer: 0.23343939
                                                0.838505
                                                0.10453451 (bias)

Input weights for perceptron 4 of hidden layer: 0.9574335
                                                0.38394147
                                                0.5190349 (bias)

Input weights for perceptron 5 of hidden layer: 0.42185658
                                                0.43678635
                                                0.21870112 (bias)

Input weights for perceptron 6 of hidden layer: 0.33765048
                                                0.8054656
                                                0.73771834 (bias)

Input weights for perceptron 7 of hidden layer: 0.5146919
                                                0.27594328
                                                0.8853167 (bias)

Input weights for perceptron 8 of hidden layer: 0.08159256
                                                0.70319873
                                                0.3568141 (bias)

Input weights for perceptron 9 of hidden layer: 0.06571615
                                                0.18605804
                                                0.88929623 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: ReLu
Input weights for perceptron 1 of output layer: 0.9630059
                                                0.8575326
                                                0.78125244
                                                0.77791035
                                                0.25187975
                                                0.32996005
                                                0.7198137
                                                0.8866715
                                                0.7471645
                                                0.54430974 (bias)

-------------------------------Test #9-------------------------------
Alpha: 0.2

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 9
Activator function in the hidden layer: Sigmoid
Input weights for perceptron 1 of hidden layer: 0.9655352
                                                0.28513002
                                                0.78432477 (bias)

Input weights for perceptron 2 of hidden layer: 0.9266561
                                                0.6226212
                                                0.55054164 (bias)

Input weights for perceptron 3 of hidden layer: 0.020260334
                                                0.9386701
                                                0.16394228 (bias)

Input weights for perceptron 4 of hidden layer: 0.048503578
                                                0.36921507
                                                0.0057679415 (bias)

Input weights for perceptron 5 of hidden layer: 0.7545895
                                                0.9012828
                                                0.75894606 (bias)

Input weights for perceptron 6 of hidden layer: 0.33606505
                                                0.38241768
                                                0.3865686 (bias)

Input weights for perceptron 7 of hidden layer: 0.30840772
                                                0.75767964
                                                0.8632566 (bias)

Input weights for perceptron 8 of hidden layer: 0.26132196
                                                3.3020973E-4
                                                0.7872427 (bias)

Input weights for perceptron 9 of hidden layer: 0.07384968
                                                0.5387072
                                                0.726225 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.41284263
                                                0.017561138
                                                0.4507066
                                                0.9088316
                                                0.8054123
                                                0.0365178
                                                0.5073574
                                                0.09945899
                                                0.19867545
                                                0.59482104 (bias)

-------------------------------Test #10-------------------------------
Alpha: 0.1

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 9
Activator function in the hidden layer: Sigmoid
Input weights for perceptron 1 of hidden layer: 0.55360377
                                                0.04318613
                                                0.25094187 (bias)

Input weights for perceptron 2 of hidden layer: 0.20040166
                                                0.8423177
                                                0.7679522 (bias)

Input weights for perceptron 3 of hidden layer: 0.47260284
                                                0.5709238
                                                0.1700089 (bias)

Input weights for perceptron 4 of hidden layer: 0.78356045
                                                0.5272854
                                                0.031136513 (bias)

Input weights for perceptron 5 of hidden layer: 0.17345262
                                                0.9924264
                                                0.37590212 (bias)

Input weights for perceptron 6 of hidden layer: 0.79361296
                                                0.611528
                                                0.044185996 (bias)

Input weights for perceptron 7 of hidden layer: 0.82040465
                                                0.05831331
                                                0.96554834 (bias)

Input weights for perceptron 8 of hidden layer: 0.94190675
                                                0.68187696
                                                0.76737803 (bias)

Input weights for perceptron 9 of hidden layer: 0.016477644
                                                0.14531904
                                                0.69821084 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.2988323
                                                0.65745825
                                                0.42079395
                                                0.86529124
                                                0.6721529
                                                0.27606988
                                                0.3191434
                                                0.41661108
                                                0.53234684
                                                0.8013217 (bias)

-------------------------------Test #11-------------------------------
Alpha: 0.2

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 9
Activator function in the hidden layer: Sigmoid
Input weights for perceptron 1 of hidden layer: 0.59024364
                                                0.0132728815
                                                0.68017787 (bias)

Input weights for perceptron 2 of hidden layer: 0.1521132
                                                0.7846455
                                                0.7264036 (bias)

Input weights for perceptron 3 of hidden layer: 0.839369
                                                0.7029153
                                                0.9065311 (bias)

Input weights for perceptron 4 of hidden layer: 0.3188877
                                                0.23907602
                                                0.5927801 (bias)

Input weights for perceptron 5 of hidden layer: 0.4423334
                                                0.26656204
                                                0.13489771 (bias)

Input weights for perceptron 6 of hidden layer: 0.13879758
                                                0.27199388
                                                0.5740497 (bias)

Input weights for perceptron 7 of hidden layer: 0.94401115
                                                0.13297057
                                                0.37904024 (bias)

Input weights for perceptron 8 of hidden layer: 0.28506553
                                                0.5788952
                                                0.64028263 (bias)

Input weights for perceptron 9 of hidden layer: 0.12773311
                                                0.32393575
                                                0.07634866 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.022467732
                                                0.85313076
                                                0.44553846
                                                0.2360493
                                                0.90360564
                                                0.659131
                                                0.47795123
                                                0.6533322
                                                0.47196478
                                                0.7446517 (bias)

-------------------------------Test #12-------------------------------
Alpha: 0.3

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 9
Activator function in the hidden layer: Sigmoid
Input weights for perceptron 1 of hidden layer: 0.7369258
                                                0.24525797
                                                0.7335238 (bias)

Input weights for perceptron 2 of hidden layer: 0.8286569
                                                0.4827016
                                                0.7774929 (bias)

Input weights for perceptron 3 of hidden layer: 0.64497536
                                                0.3145687
                                                0.4665314 (bias)

Input weights for perceptron 4 of hidden layer: 0.18744576
                                                0.19268954
                                                0.83725756 (bias)

Input weights for perceptron 5 of hidden layer: 0.5752742
                                                0.35596704
                                                0.85670143 (bias)

Input weights for perceptron 6 of hidden layer: 0.34210974
                                                0.24064308
                                                0.70162874 (bias)

Input weights for perceptron 7 of hidden layer: 0.9621821
                                                0.80292445
                                                0.13230973 (bias)

Input weights for perceptron 8 of hidden layer: 0.24894232
                                                0.13788724
                                                0.1532675 (bias)

Input weights for perceptron 9 of hidden layer: 0.4383431
                                                0.9843246
                                                0.8781195 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.17793566
                                                0.5922134
                                                0.9873261
                                                0.57252353
                                                0.784301
                                                0.8722206
                                                0.20685601
                                                0.9154893
                                                0.35979825
                                                0.33425462 (bias)

-------------------------------Test #13-------------------------------
Alpha: 0.4

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 9
Activator function in the hidden layer: Sigmoid
Input weights for perceptron 1 of hidden layer: 0.82665646
                                                0.6692582
                                                0.60579777 (bias)

Input weights for perceptron 2 of hidden layer: 0.8921388
                                                0.16818601
                                                0.53902066 (bias)

Input weights for perceptron 3 of hidden layer: 0.019221902
                                                0.6382901
                                                0.15800679 (bias)

Input weights for perceptron 4 of hidden layer: 0.48273504
                                                0.49029732
                                                0.6844422 (bias)

Input weights for perceptron 5 of hidden layer: 0.8569794
                                                0.19962221
                                                0.42561817 (bias)

Input weights for perceptron 6 of hidden layer: 0.426988
                                                0.14683247
                                                0.27339953 (bias)

Input weights for perceptron 7 of hidden layer: 0.53665876
                                                0.17872345
                                                0.98955286 (bias)

Input weights for perceptron 8 of hidden layer: 0.7283914
                                                0.798633
                                                0.8582608 (bias)

Input weights for perceptron 9 of hidden layer: 0.037740886
                                                0.5620878
                                                0.41046858 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.6985894
                                                0.23786074
                                                0.9662199
                                                0.49528825
                                                0.95022184
                                                0.42660242
                                                0.5322612
                                                0.88947695
                                                0.9633616
                                                0.91279614 (bias)

-------------------------------Test #14-------------------------------
Alpha: 0.5

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 9
Activator function in the hidden layer: Sigmoid
Input weights for perceptron 1 of hidden layer: 0.80620813
                                                0.14241385
                                                0.78489316 (bias)

Input weights for perceptron 2 of hidden layer: 0.11331034
                                                0.15054524
                                                0.67821133 (bias)

Input weights for perceptron 3 of hidden layer: 0.47888935
                                                0.66612226
                                                0.5619207 (bias)

Input weights for perceptron 4 of hidden layer: 0.5052188
                                                0.41849595
                                                0.79489636 (bias)

Input weights for perceptron 5 of hidden layer: 0.678375
                                                0.73929036
                                                0.83485454 (bias)

Input weights for perceptron 6 of hidden layer: 0.41630065
                                                0.65876913
                                                0.24714923 (bias)

Input weights for perceptron 7 of hidden layer: 0.06610948
                                                0.37536782
                                                0.22412974 (bias)

Input weights for perceptron 8 of hidden layer: 0.8671206
                                                0.89465904
                                                0.29965448 (bias)

Input weights for perceptron 9 of hidden layer: 0.6421156
                                                0.5286592
                                                0.48704088 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.36149085
                                                0.72747797
                                                0.9823139
                                                0.73029673
                                                0.43721873
                                                0.19119364
                                                4.13239E-4
                                                0.745751
                                                0.7064035
                                                0.14896673 (bias)

-------------------------------Test #15-------------------------------
Alpha: 0.6

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 9
Activator function in the hidden layer: Sigmoid
Input weights for perceptron 1 of hidden layer: 0.20240122
                                                0.9625442
                                                0.0040967464 (bias)

Input weights for perceptron 2 of hidden layer: 0.25404
                                                0.5469015
                                                0.9805483 (bias)

Input weights for perceptron 3 of hidden layer: 0.7797366
                                                0.039326787
                                                0.9007399 (bias)

Input weights for perceptron 4 of hidden layer: 0.85625255
                                                0.8957435
                                                0.11951381 (bias)

Input weights for perceptron 5 of hidden layer: 0.6797846
                                                0.51028997
                                                0.9885356 (bias)

Input weights for perceptron 6 of hidden layer: 0.79205334
                                                0.21490806
                                                0.8615637 (bias)

Input weights for perceptron 7 of hidden layer: 0.94600195
                                                0.2220326
                                                0.7051655 (bias)

Input weights for perceptron 8 of hidden layer: 0.63060784
                                                0.9488599
                                                0.12985653 (bias)

Input weights for perceptron 9 of hidden layer: 0.6360704
                                                0.033727765
                                                0.5536985 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.03174752
                                                0.4566911
                                                0.8268213
                                                0.44308084
                                                0.7649683
                                                0.2515254
                                                0.8482377
                                                0.7682098
                                                0.48751807
                                                0.15926987 (bias)

-------------------------------Test #16-------------------------------
Alpha: 0.70000005

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 9
Activator function in the hidden layer: Sigmoid
Input weights for perceptron 1 of hidden layer: 0.046753228
                                                0.8253454
                                                0.5574505 (bias)

Input weights for perceptron 2 of hidden layer: 0.77467847
                                                0.35384285
                                                0.6807425 (bias)

Input weights for perceptron 3 of hidden layer: 0.23853844
                                                0.7138727
                                                0.41038233 (bias)

Input weights for perceptron 4 of hidden layer: 0.2641278
                                                0.7957332
                                                0.3055845 (bias)

Input weights for perceptron 5 of hidden layer: 0.8764843
                                                0.5048435
                                                0.99735427 (bias)

Input weights for perceptron 6 of hidden layer: 0.8667996
                                                0.54855984
                                                0.048842013 (bias)

Input weights for perceptron 7 of hidden layer: 0.2084871
                                                0.8036957
                                                0.11375821 (bias)

Input weights for perceptron 8 of hidden layer: 0.19640547
                                                0.766222
                                                0.40098995 (bias)

Input weights for perceptron 9 of hidden layer: 0.07059878
                                                0.48996943
                                                0.7451491 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.53705406
                                                0.07045406
                                                0.5640599
                                                0.27147847
                                                0.05930817
                                                0.46158105
                                                0.9880162
                                                0.8632908
                                                0.17803389
                                                0.8920536 (bias)

-------------------------------Test #17-------------------------------
Alpha: 0.8000001

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 9
Activator function in the hidden layer: Sigmoid
Input weights for perceptron 1 of hidden layer: 0.40719903
                                                0.49955493
                                                0.96334714 (bias)

Input weights for perceptron 2 of hidden layer: 0.803814
                                                0.45420247
                                                0.6740716 (bias)

Input weights for perceptron 3 of hidden layer: 0.7119052
                                                0.3889649
                                                0.45446754 (bias)

Input weights for perceptron 4 of hidden layer: 0.94157594
                                                0.851907
                                                0.26032126 (bias)

Input weights for perceptron 5 of hidden layer: 0.09910542
                                                0.748908
                                                0.9452347 (bias)

Input weights for perceptron 6 of hidden layer: 0.77042836
                                                0.13673753
                                                0.63259923 (bias)

Input weights for perceptron 7 of hidden layer: 0.60358965
                                                0.7951859
                                                0.8215789 (bias)

Input weights for perceptron 8 of hidden layer: 0.6444745
                                                0.7223013
                                                0.6333259 (bias)

Input weights for perceptron 9 of hidden layer: 0.34008163
                                                0.5499162
                                                0.1474616 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.98131806
                                                0.36604726
                                                0.20429808
                                                0.15118712
                                                0.20807523
                                                0.23576188
                                                0.34763938
                                                0.12281269
                                                0.90190154
                                                0.78353864 (bias)

-------------------------------Test #18-------------------------------
Alpha: 0.9000001

Number of perceptrons in the input layer: 2

Number of perceptrons in the hidden layer: 9
Activator function in the hidden layer: Sigmoid
Input weights for perceptron 1 of hidden layer: 0.97397834
                                                0.06475675
                                                0.9793993 (bias)

Input weights for perceptron 2 of hidden layer: 0.9480701
                                                0.73763216
                                                0.18020993 (bias)

Input weights for perceptron 3 of hidden layer: 0.92935556
                                                0.0898329
                                                0.15023434 (bias)

Input weights for perceptron 4 of hidden layer: 0.24080455
                                                0.93500733
                                                0.71978056 (bias)

Input weights for perceptron 5 of hidden layer: 0.070608556
                                                0.7055258
                                                0.9588225 (bias)

Input weights for perceptron 6 of hidden layer: 0.10331452
                                                0.436621
                                                0.048868418 (bias)

Input weights for perceptron 7 of hidden layer: 0.2844864
                                                0.764147
                                                0.48965585 (bias)

Input weights for perceptron 8 of hidden layer: 0.17200583
                                                0.57790136
                                                0.8831214 (bias)

Input weights for perceptron 9 of hidden layer: 0.66775244
                                                0.40156984
                                                0.50132227 (bias)


Number of perceptrons in the output layer: 1
Activator function in the output layer: Sigmoid
Input weights for perceptron 1 of output layer: 0.9480994
                                                0.85628057
                                                0.7709348
                                                0.8926647
                                                0.5413649
                                                0.9804114
                                                0.84521896
                                                0.15554094
                                                0.16199136
                                                0.6685242 (bias)

